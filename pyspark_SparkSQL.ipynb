{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1uZiSS_kkL9MEa3IY1XCfCuW1vxw_YQDD",
      "authorship_tag": "ABX9TyPoG7603TULCnCjD3KlWP4P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cruz-marco/pyspark_course/blob/main/pyspark_SparkSQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "qfHMMFM-LpIZ"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.3-bin-hadoop3.2'\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spark SQL\n",
        "\n",
        "- O Spark permite també utilizar tabelas dentro de banco de dados um ambiente que se assemelha ao de um banco de dados relacional comum (Postgres, MySQL)\n",
        "\n",
        "- As tabelas são exatamente iguais em estrutura e funcionalidades aos bancos de dado SQL. Diferindo dos DataFrames, as tabelas são objetos que persistem depois que a sessão do Spark é finalizada.\n",
        "\n",
        "- Tabelas e DataFrames são totalmente interoperáveis no ambiente Spark, logo, podemos fazer o cast de um tipo de estrutura para outro sem muito problema.\n",
        "\n",
        "- Apesar de sermos completamente livres para fazer consultas e gerenciamento de bancos de dados e tabelas pelo spark, o retorno no framework do PySpark é sempre em um objeto DataFrame, o que não significa que a tabela seja mutável, mas que a saída daquela consulta usa o tipo de dado DataFrame.\n",
        "\n",
        "## 1. Tipos de Tabelas:\n",
        "> **Gerenciadas (Managed)**: Spark gerendia os dados e os metadados deta tabela. Se apagarmos esta tabela, ela desaparece por completo (dados e metadados desaparecem). Ficam armazenadas no Warehouse do Spark.\n",
        "\n",
        "> **Não Gerenciadas (External)**: É uma tabelo onde apenas os metadados são gerenciados pelo Spark e os dados em si estão localizados um um ludar diferente do Spark Warehouse.\n",
        "\n",
        "## 2. Views:\n",
        "As views são apelidos que podemos dar a uma tabela que é criada a partir de uma consulta. Usando joins ou criando novas colunas com os dados, por exemplo. O uso de views facilita o trabalho quando precisamos executar uma consulta mais complexa e que necessita de um aninhamento de selects, por exemplo.\n",
        "### Podem ser de dois tipos:\n",
        "> **Globais**: Visíveis em todas as sessões.\n",
        "\n",
        "> **Sessão**: Visíveis apenas na sessão atual, sendo destruídas quando a mesma é encerrada.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n12k3ojLTE2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando o dataframe despachantes para ser usado nos exemplos a seguir."
      ],
      "metadata": {
        "id": "ASkO2z0aWxzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "arqschema = \"id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING\"\n",
        "\n",
        "despachantes = spark.read.csv(\"/content/drive/MyDrive/Datasets/pyspark_course/despachantes.csv\", \n",
        "                              header=False, schema=arqschema)"
      ],
      "metadata": {
        "id": "Egn-CzhaLz-q"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mostrando e criando os bancos de dados."
      ],
      "metadata": {
        "id": "O1VvAYibW62_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show databases').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ipkFnEN5PwZS",
        "outputId": "f35c0267-8f40-49bf-fff2-96bbcd373e91"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('create database desp').show() # Criando o banco de dados DESP"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XecLutH0QUuI",
        "outputId": "d1cf5f36-1c08-4c51-9048-0f64641b1b21"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "++\n",
            "||\n",
            "++\n",
            "++\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show databases').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LB8ayVYxQhtl",
        "outputId": "b2ce377f-3fca-4818-ccdf-a52d008f6f60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|namespace|\n",
            "+---------+\n",
            "|  default|\n",
            "|     desp|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('use desp') # Selecionando o banco de dados DESP para ser usado."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dO5XEZMZQmQ9",
        "outputId": "8f63f9ec-f0c9-4767-9775-13257c9e6123"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Com o banco de dados criado e selecionado, podemos escrever o DataFrame *despachantes* completo como uma tabela dentro do banco de dados DESP com apenas umalinha de código."
      ],
      "metadata": {
        "id": "WQVdYBbnXI6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.write.saveAsTable(\"Despachantes\") "
      ],
      "metadata": {
        "id": "3b7uMySRQuMG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mostrando a recém criada **TABELA** Despachantes usando um select clássico SQL."
      ],
      "metadata": {
        "id": "Ww6yEnIvXbni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('select * from Despachantes').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPdTWRx5RTpR",
        "outputId": "5b31571b-97a0-42c4-92e8-3d9bff1bf255"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------------------+------+-------------+------+----------+\n",
            "| id|               nome|status|       cidade|vendas|      data|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "|  1|   Carminda Pestana| Ativo|  Santa Maria|    23|2020-08-11|\n",
            "|  2|    Deolinda Vilela| Ativo|Novo Hamburgo|    34|2020-03-05|\n",
            "|  3|   Emídio Dornelles| Ativo| Porto Alegre|    34|2020-02-05|\n",
            "|  4|Felisbela Dornelles| Ativo| Porto Alegre|    36|2020-02-05|\n",
            "|  5|     Graça Ornellas| Ativo| Porto Alegre|    12|2020-02-05|\n",
            "|  6|   Matilde Rebouças| Ativo| Porto Alegre|    22|2019-01-05|\n",
            "|  7|    Noêmia   Orriça| Ativo|  Santa Maria|    45|2019-10-05|\n",
            "|  8|      Roque Vásquez| Ativo| Porto Alegre|    65|2020-03-05|\n",
            "|  9|      Uriel Queiroz| Ativo| Porto Alegre|    54|2018-05-05|\n",
            "| 10|   Viviana Sequeira| Ativo| Porto Alegre|     0|2020-09-05|\n",
            "+---+-------------------+------+-------------+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mostrando as tabelas contidas no banco de dados Desp"
      ],
      "metadata": {
        "id": "KCqALQcMXrId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql('show tables').show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw5GgJUqRcet",
        "outputId": "448b8852-5b7c-42ef-e23e-b0261cc662bb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+------------+-----------+\n",
            "|namespace|   tableName|isTemporary|\n",
            "+---------+------------+-----------+\n",
            "|     desp|despachantes|      false|\n",
            "+---------+------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inserindo o método \"*.mode()*\" podemos modificar o modo de escrita da tabela. Por exemplo, usando OVERWRITE para sobrescrever ou APPEND para adicionar mais linhas mantendo as que já estão persistidas na tabela.."
      ],
      "metadata": {
        "id": "wwEojlXPXxGx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.write.mode('overwrite').saveAsTable(\"Despachantes\")"
      ],
      "metadata": {
        "id": "E5whO1GARibg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBW0hRm9R6Zn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}