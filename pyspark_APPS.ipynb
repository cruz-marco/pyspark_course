{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Qdm6TL-5x8obAZwP1geSSqI_bgP8NoWO",
      "authorship_tag": "ABX9TyMdXNBDGArgEmaSYyMyqENB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cruz-marco/pyspark_course/blob/main/pyspark_APPS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EiUDaSA7DBZB"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.3-bin-hadoop3.2'\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Criando APPS"
      ],
      "metadata": {
        "id": "swub4t8mKmF4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aplicativo.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  spark = SparkSession.builder.appName('Exemplo').getOrCreate()\n",
        "\n",
        "  arqschema = (\"id INT, nome STRING, status STRING, cidade STRING, vendas INT,\"\n",
        "              \" data TIMESTAMP\")\n",
        "\n",
        "  despachantes = spark.read.load((\"/content/drive/MyDrive/Datasets/pyspark_cou\"\n",
        "                            \"rse/despachantes.csv\"), header=False, format=\"csv\", \n",
        "                            schema=arqschema)\n",
        "\n",
        "  calculo = despachantes.select(f.year(f.col('data')).alias('anos'), f.col('id'))\\\n",
        "            .groupBy('anos')\\\n",
        "            .agg(f.count('id').alias('count'))\\\n",
        "            .orderBy(f.col('count').desc())\n",
        "\n",
        "  calculo.write.format('console').save()\n",
        "  spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViDyD8JvKh8N",
        "outputId": "02ad1615-4ac9-484d-cbcc-dbf54ddae80c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aplicativo.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aplicativo2.py\n",
        "import sys\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  spark = SparkSession.builder.appName('Exemplo').getOrCreate()\n",
        "\n",
        "  arqschema = (\"id INT, nome STRING, status STRING, cidade STRING, vendas INT,\"\n",
        "              \" data TIMESTAMP\")\n",
        "\n",
        "  despachantes = spark.read.load(sys.argv[1], header=False, format=\"csv\", \n",
        "                                schema=arqschema)\n",
        "\n",
        "  calculo = despachantes.select(f.year(f.col('data')).alias('anos'), f.col('id'))\\\n",
        "            .groupBy('anos')\\\n",
        "            .agg(f.count('id').alias('count'))\\\n",
        "            .orderBy(f.col('count').desc())\n",
        "\n",
        "  calculo.write.format('console').save()\n",
        "  spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaUsgFHrM1Gw",
        "outputId": "edf55e99-ee72-4939-b696-3f5cb710408d"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aplicativo2.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile aplicativo3.py\n",
        "import sys, getopt\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as f\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  spark = SparkSession.builder.appName('Exemplo').getOrCreate()\n",
        "\n",
        "  opts, args = getopt.getopt(sys.argv[1:], 't:i:o')\n",
        "  formato, infile, outdir = '', '', ''\n",
        "\n",
        "  for opt, arg in opts:\n",
        "    if opt == '-t':\n",
        "      formato = arg\n",
        "    elif opt == '-i':\n",
        "      infile = arg\n",
        "    elif opt == '-o':\n",
        "      outdir = arg\n",
        "\n",
        "  \n",
        "  data = spark.read.csv(infile, header=False, inferSchema=True)\n",
        "\n",
        "  data.write.format(formato).save(outdir)\n",
        "\n",
        "  spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zUYKbWjSfFz",
        "outputId": "9b7f21c7-92fb-4b42-b237-2bf2ec2dae76"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing aplicativo3.py\n"
          ]
        }
      ]
    }
  ]
}