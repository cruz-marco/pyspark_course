{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e4l4MNwqBNYt"
      ],
      "mount_file_id": "1X8PfmED3dfL72WQjQn4OyRJUOCDJNSZy",
      "authorship_tag": "ABX9TyMs8v7VARoJ/I41L2h2PTWs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cruz-marco/pyspark_course/blob/main/pyspark_DataFrame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instalação e configuração Spark"
      ],
      "metadata": {
        "id": "e4l4MNwqBNYt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPKFS4-M_m3v"
      },
      "outputs": [],
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.3/spark-3.2.3-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.3-bin-hadoop3.2.tgz\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\" \n",
        "os.environ[\"SPARK_HOME\"] = '/content/spark-3.2.3-bin-hadoop3.2'\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataFrames\n",
        "\n",
        "- Tabelas com linhas e colunas;\n",
        "- Imutáveis;\n",
        "- Schema conhecido;\n",
        "- Linhagem Preservada;\n",
        "- Colunas podem ter tipos diferentes;\n",
        "- Podemos agrupar, ordenar e filtrar;\n",
        "- Spark otimiza análises usando planos de execução (DAG's)\n",
        "\n",
        "## Lazy Evaluation\n",
        "> O processamento da transformação só ocorre quando há uma ação: \n",
        "\n",
        "## Ações:\n",
        "> (reduce, collect, count, first, take, takeSample, takeOrdered, saveAsTestFile, saveAsSequenceFile, saveAsObjectFile, countByKey, foreach)\n",
        "\n",
        "## Transformações:\n",
        "> (map, filter, flatMap, mapPartitions, mapPartitionsWithIndex, sample, union, intersection, distinct, groupByKey, reduceByKey, aggregateByKey, sortByKey, join, cogroup, cartesian, pipe, coalesce, repartition, repartitionAndSortWithinPartitions)\n"
      ],
      "metadata": {
        "id": "NfYXPqPABlN-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando um [DataFrame](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.html#pyspark.sql.DataFrame) de exemplo:\n",
        "- [spark.createDataFrame()](https://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.SparkSession.createDataFrame.html)"
      ],
      "metadata": {
        "id": "6Fl7TU3g0En0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [(\"Pedro\", 10), (\"Maria\", 20), (\"José\", 40)] #Dados a serem inseridos na tabela\n",
        "df1 = spark.createDataFrame(data) #instanciando o DataFrame\n",
        "df1.show() #Comando para mostar o frame, pode recer um parâmetro com o número."
      ],
      "metadata": {
        "id": "KHayJtLSBkNe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "schema = \"Id INT, Nome STRING\" #definindo um schema a ser usado no DataFrame\n",
        "data_2 = [(1, \"Pedro\"), (2, \"Maria\")]\n",
        "\n",
        "df2 = spark.createDataFrame(data_2, schema=schema)\n",
        "\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "hSxHM0wLyCHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pacote de [funções](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html):"
      ],
      "metadata": {
        "id": "iFTQ5U6n2vlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as f #Importando a biblioteca de funções\n",
        "schema_2 = \"produtos STRING, vendas INT\"\n",
        "vendas = [(\"Caneta\", 10), (\"Lápis\", 20), (\"Caneta\", 40)]\n",
        "\n",
        "df3 = spark.createDataFrame(vendas, schema_2)\n",
        "\n",
        "df3.show()"
      ],
      "metadata": {
        "id": "oGEHfXFW1ari"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Diferente do Pandas, as funções ficam em um pacote a parte, \n",
        "logo, temos que fazer o import ou do pacote, ou da função \n",
        "específica. Por exemplo, a sum() utilizada com o método de \n",
        "agregação, conforme mostrado abaixo.\n",
        "\"\"\"\n",
        "\n",
        "agrupado = df3.groupBy(\"produtos\")\\\n",
        "            .agg(f.sum(\"vendas\"))\n",
        "\n",
        "agrupado.show()"
      ],
      "metadata": {
        "id": "JcX8pjOm2Ki8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.select(\"produtos\").show() # Selecionando uma única coluna no DataFrame, por exemplo."
      ],
      "metadata": {
        "id": "WeUhhH5e2hos"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3.select(\"produtos\", \"vendas\", f.expr(\"vendas * 0.2\")).show() \n",
        "\"\"\"\n",
        "A Função EXPR cria uma expressão que pode ser usada para criar mais uma linha\n",
        "no dataframe, enriquecendo a nossa análise. Ela recebe uma string com a\n",
        "expressão a ser processada.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "O04dJePN3Z5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(\n",
        "    df3.schema, # ver a estrutura das colunas\n",
        "    df3.columns # ver os nomes das colunas\n",
        ")"
      ],
      "metadata": {
        "id": "Kj-y3RuZ41mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carregando dados de duas maneiras distintas no Spark (DataFrames)"
      ],
      "metadata": {
        "id": "b6zJZUVCtQqB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o spark.read.csv:"
      ],
      "metadata": {
        "id": "xsE-IJuStaNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "arqschema = \"id INT, nome STRING, status STRING, cidade STRING, vendas INT, data STRING\"\n",
        "# esquema de colunas, colocando seu nome e tipo, em sequência, para ser usado\n",
        "# como referência na leitura dos dados em CSV, para podermos escolher\n",
        "# os tipos de dados e o nome das colunas, visto que o CSV em questão não possui\n",
        "# cabeçalho.\n",
        "\n",
        "despachantes = spark.read.csv(\"/content/drive/MyDrive/Datasets/pyspark_course/despachantes.csv\", \n",
        "                              header=False, schema=arqschema)\n"
      ],
      "metadata": {
        "id": "6ppETgnpN1hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.show()"
      ],
      "metadata": {
        "id": "f9TY1ZCVb1jm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.groupBy(\"cidade\")\\\n",
        ".agg(f.sum(\"vendas\")).show()\n",
        "# Exemplo de groupBy"
      ],
      "metadata": {
        "id": "ENJ2HAvYb7WM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando o spark.read.load:"
      ],
      "metadata": {
        "id": "LAwMwgc_u7lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "desp_autoschema = spark.read.load(\"/content/drive/MyDrive/Datasets/pyspark_course/despachantes.csv\",\n",
        "                                  header=False, format=\"csv\", sep=\",\",\n",
        "                                  inferSchema=True)\n",
        "# forma um tanto mais sucinta, é necessário informar o formato e permitir o \n",
        "# parâmetro inferSchema (valor True) para que o próprio spark deduza o tipo\n",
        "# de dados sozinho. É uma boa prática informar qual o tipo de separador SEP,\n",
        "# pois no Brasil, usamos a vírgula para declarar a parte decimal de um número\n",
        "# não inteiro."
      ],
      "metadata": {
        "id": "eFFlm09ZcPIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "desp_autoschema.show()\n",
        "# como o arquivo não tem cabeçalho, o spark nomeia as colunas automaticamente."
      ],
      "metadata": {
        "id": "EjCUntkRczTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(\n",
        "    despachantes.schema, # comparação dos schemas declarados e inferidos.\n",
        "    \"------------------\",\n",
        "    desp_autoschema.schema\n",
        ")\n"
      ],
      "metadata": {
        "id": "D0NLwA_Sc2wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fazendo consultas\n",
        "\n",
        "> Select, Where, OrderBy, Distinct são cláusulas SQL que no PySpark são métodos do objeto DataFrame, diferentemente do pandas, temos que usar o SELECT para selecionar as colunas que nos interessam, no lugar de exibirmos todas."
      ],
      "metadata": {
        "id": "pwcKnFLnv7P0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.select(\"id\", \"nome\", \"vendas\")\\\n",
        "        .where(f.col(\"vendas\") > 20)\\\n",
        "        .show()"
      ],
      "metadata": {
        "id": "BVdYdfh0dR1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> A cláusula WHERE deve ser usada juntamente com a função COL do pacote de funções SQL do PySpark. As condições podem ser conectadas usando \"&\" e \"|\", e uma expressão pode ser negada usando \"~\"."
      ],
      "metadata": {
        "id": "RPiSPaUGwdVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.select(\"id\", \"nome\", \"vendas\")\\\n",
        "        .where((f.col(\"vendas\") > 20) & (f.col(\"vendas\") < 40))\\\n",
        "        .show()"
      ],
      "metadata": {
        "id": "3gu_t-qWfKIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Renomeando colunas:\n",
        "> Diferente do pandas, onde podemos renomear diversas colunas passando um dicionário como parâmetro para o método da classe DataFrame; as colunas do DataFrame do PySpark devem ser renomeadas uma a uma. Portanto, faz-se necessário o uso de um loop, caso queiramos renomear mais de uma de forma ágil, por exemplo."
      ],
      "metadata": {
        "id": "C4QAu-GGxG-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#renomeando todas as colunas de uma só vez numa nova variável\n",
        "# novo_desp\n",
        "novo_desp = desp_autoschema\n",
        "for i in list(zip(desp_autoschema.columns, despachantes.columns)):\n",
        "  novo_desp = novo_desp.withColumnRenamed(*i)\n",
        "\n",
        "novo_desp.show()"
      ],
      "metadata": {
        "id": "xFVfoxvifjug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando uma nova coluna com os dados da coluna data com o tipo timestamp"
      ],
      "metadata": {
        "id": "CU7Q7W3rzw3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp = novo_desp.withColumn(\"data2\", f.to_timestamp(f.col(\"data\"), \n",
        "                                \"yyyy-MM-dd\"))\n",
        "novo_desp.show()"
      ],
      "metadata": {
        "id": "X6RyQMGxnu_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp.schema"
      ],
      "metadata": {
        "id": "NYA8nTgFo5Fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Selecionando os anos das datas (str e timestamp) e os nomes dos despachantes, ordenando por nome."
      ],
      "metadata": {
        "id": "2AJ5qLqF0DVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp.select(f.year(\"data\"), f.year(\"data2\"), \"nome\")\\\n",
        "                .distinct()\\\n",
        "                .orderBy(\"nome\")\\\n",
        "                .show()"
      ],
      "metadata": {
        "id": "RTaA_mcMpBTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Usando o ALIAS, podemos dar apelidos para todas as colunas, logo, podemos criar um agrupamento usando as funções do pacote functions e as apelidando e usando a referência do apelido em outros métodos do DataFrame como o groupBy e o orderBy.\n",
        "\n",
        "> Para ordenar crescente o decrescente, usamos .asc() ou .desc(), sendo estes métodos para a função COL do pacote de funções SQL.|"
      ],
      "metadata": {
        "id": "nqkajluI0qLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp.select(f.year(\"data\").alias(\"anos\"))\\\n",
        "                .groupBy(\"anos\")\\\n",
        "                .agg(f.count(f.col(\"anos\")).alias(\"ocorr\"))\\\n",
        "                .orderBy(f.col(\"ocorr\").desc())\\\n",
        "                .show()#Só funciona se dermos um apelido para a coluna"
      ],
      "metadata": {
        "id": "Q5t7Dyl0psIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Total de vendas"
      ],
      "metadata": {
        "id": "5giH2hGa0nYK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp.select(f.sum(\"vendas\")).show()"
      ],
      "metadata": {
        "id": "KqgeqXFAqBSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principais Transformações e Ações:"
      ],
      "metadata": {
        "id": "DK8Mw3mv4ZNI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.take(5)"
      ],
      "metadata": {
        "id": "4Jgk8-nWtLPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.collect() #ação herdada do RDD"
      ],
      "metadata": {
        "id": "uIoo6nZr4rik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.count() #sem nenhum argumento, retorna o número de linhas."
      ],
      "metadata": {
        "id": "uxC2lV6M4zOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Podemos ordenar por duas colunas usando a função col com o método asc ou desc"
      ],
      "metadata": {
        "id": "GqU3oVf4C5fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.orderBy(f.col(\"cidade\").asc(),\n",
        "                     f.col(\"vendas\").desc())\\\n",
        "                     .show()"
      ],
      "metadata": {
        "id": "vKn3w94v48ri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Tabela de agrupamento retornando a quantidade de vendas das cidades da maior para a menor."
      ],
      "metadata": {
        "id": "3h2J1HAsDcuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.groupBy(\"cidade\")\\\n",
        "            .agg(f.sum(\"vendas\").alias(\"vendas_totais\"))\\\n",
        "            .orderBy(f.col(\"vendas_totais\").desc())\\\n",
        "            .show()"
      ],
      "metadata": {
        "id": "0hc_frf_59E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Filtrando o DataFrame inteiro a partir de uma coluna."
      ],
      "metadata": {
        "id": "dTLEEa-MDoAF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "despachantes.filter(f.col(\"cidade\") != \"Porto Alegre\").show()"
      ],
      "metadata": {
        "id": "0sNt54Yr6tiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Escrita e Carregamento de Arquivos"
      ],
      "metadata": {
        "id": "zpzc-3ZsDwE8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Salvando o novo_desp em arquivo parquet no Drive. O Arquivo Parquet salva o cabeçalho e os tipos de dados automaticamente."
      ],
      "metadata": {
        "id": "KouajXWBD1W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "novo_desp.write.format(\"parquet\").save(\"/content/drive/MyDrive/Datasets/pyspark_course/dfpqt\")"
      ],
      "metadata": {
        "id": "6Bj3uACp75gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Importando e testando o arquivo criado:"
      ],
      "metadata": {
        "id": "zW29FrgMEJ5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "imported_df = spark.read.load(\"/content/drive/MyDrive/Datasets/pyspark_course/dfpqt/part-00000-c8a74511-ecb8-4fc4-91ba-77fe818d7065-c000.snappy.parquet\",\n",
        "                              format=\"parquet\")"
      ],
      "metadata": {
        "id": "-ALDa1Of-qYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imported_df.show()"
      ],
      "metadata": {
        "id": "hlIs9PrDCmqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imported_df.schema"
      ],
      "metadata": {
        "id": "2oVmxp9LCpFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qhBWU2whDWlj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}